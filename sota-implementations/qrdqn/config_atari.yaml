# QR-DQN Configuration for Atari
# Reference: "Distributional Reinforcement Learning with Quantile Regression"
# https://arxiv.org/abs/1710.10044

device: null  # Auto-detect (cuda if available)

# Environment
env:
  env_name: ALE/Pong-v5
  backend: gymnasium

# Collector
collector:
  total_frames: 40_000_100
  frames_per_batch: 1600
  eps_start: 1.0
  eps_end: 0.01
  annealing_frames: 4_000_000
  init_random_frames: 200_000

# Replay Buffer
buffer:
  buffer_size: 1_000_000
  batch_size: 32
  scratch_dir: null

# Logger
logger:
  backend: wandb
  project_name: torchrl_example_qrdqn
  group_name: null
  exp_name: QR-DQN
  test_interval: 1_000_000
  num_test_episodes: 3
  video: False

# Optimizer
optim:
  lr: 0.00005  # Lower LR than DQN as per paper
  max_grad_norm: 10
  adam_eps: 0.01 / 32  # eps / batch_size as per paper

# Loss
loss:
  gamma: 0.99
  hard_update_freq: 10_000
  num_updates: 100
  # QR-DQN specific
  num_quantiles: 200  # N in paper (200 for Atari)
  huber_kappa: 1.0    # Huber loss threshold
  double_qrdqn: False  # Whether to use Double DQN action selection

# Compile (torch.compile and cudagraphs)
compile:
  compile: False
  compile_mode: default
  cudagraphs: False
