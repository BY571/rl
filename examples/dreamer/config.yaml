env:
  name: cheetah
  task: run
  backend: dm_control
  catframes: 1
  record_video: 0
  frame_skip: 2
  from_pixels: True
  grayscale: False
  image_size : 64
  batch_transform: 1
  # probably not needed vvvv
  normalize_rewards_online: True
  normalize_rewards_online_scale: 5.0
  normalize_rewards_online_decay: 0.99999
  reward_scaling: 1.0

collector:
  async_collection: True
  total_frames: 5000000
  init_env_steps: 1000
  init_random_frames: 5000
  max_frames_per_traj: 1000

  env_per_collector: 8
  num_workers: 8
  collector_device: cuda:1
  frames_per_batch: 800

optimization:
  grad_clip: 100
  batch_size: 50
  batch_length: 50

  world_model_lr: 6e-4
  actor_value_lr: 8e-5
  optim_steps_per_batch: 80


# we want 50 frames / traj in the replay buffer. Given the frame_skip=2 this makes each traj 100 steps long
networks:
  state_dim: 30
  rssm_hidden_dim: 200

replay_buffer: 
  buffer_size: 20000


logger:
  offline_logging: False
  logger: csv
  record_interval: 30
  record_frames: 1000
